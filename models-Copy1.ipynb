{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpqG3V19PkS3"
   },
   "outputs": [],
   "source": [
    "# Import supporting libraries\n",
    "import nbimporter\n",
    "from Data_cleaning_and_preprocessing import *\n",
    "import pandas \n",
    "import numpy\n",
    "import sklearn.impute as impute\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.model_selection as modelsel\n",
    "import sklearn.tree as tree\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.svm as SVC\n",
    "import pydotplus\n",
    "import collections\n",
    "import sklearn\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "from sklearn.metrics import  ConfusionMatrixDisplay\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: Thinh Le (Random Forest Classifier all methods)\n",
    "if model == 11: # This is Random Forest Classifier model without balancing technique\n",
    "  # Radhey: The optimized parameters can be seen in model == 12, you can use that model to work on player rating\n",
    "  classifier = sklearn.ensemble.RandomForestClassifier(n_estimators = 2, criterion = 'gini', max_depth = 5, min_samples_leaf = 1)\n",
    "  selected_features=feature_sel(classifier,3,X,y)\n",
    "  X_selected=X[selected_features]\n",
    "  one_hot_X=pd.get_dummies(X_selected,columns=selected_features)\n",
    "  X_rf=one_hot_X.to_numpy()\n",
    "\n",
    "  # Partition into training/ test/ validation\n",
    "  RANSEED = 10000\n",
    "  X_train, X_test, y_train, y_test = modelsel.train_test_split(X_rf, y, test_size = 0.3, random_state = RANSEED)\n",
    "\n",
    "  # Defining possible values of parameters \n",
    "  parameters = {'min_samples_leaf': [1, 2], \n",
    "                'n_estimators': [100, 300, 500],\n",
    "                'max_depth': [3, 4, 5],\n",
    "                'criterion': ['gini', 'entropy']} \n",
    "  RFC = sklearn.ensemble.RandomForestClassifier()\n",
    "  classifier = GridSearchCV(RFC, parameters)\n",
    "  # Fitting the model \n",
    "  classifier.fit(X_train, y_train)\n",
    "  # print best parameter \n",
    "  print(classifier.best_params_)\n",
    "  # print model\n",
    "  print(classifier.best_estimator_)\n",
    "  # Measure model performance\n",
    "  y_model = classifier.predict(X_test)\n",
    "  print(classification_report(y_test, y_model)) # print classification report\n",
    "  print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "  print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "  print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "  print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "  print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "  print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model, normalize = \"true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYeCe427PLS3"
   },
   "outputs": [],
   "source": [
    "if model == 12: # This is Random Forest Classifier model with oversampling technique\n",
    "  # Feature selection and onehot encoding for Random Forest Classifier\n",
    "  \n",
    "  classifier = sklearn.ensemble.RandomForestClassifier(n_estimators = 2, criterion = 'gini', max_depth = 3, min_samples_leaf = 1)\n",
    "  selected_features=feature_sel(classifier,3,X_oversample,y_oversample)\n",
    "  X_selected=X_oversample[selected_features]\n",
    "  one_hot_X=pd.get_dummies(X_selected,columns=selected_features)\n",
    "  X_rf=one_hot_X.to_numpy()\n",
    "  # Partition into training/ test/ validation\n",
    "  RANSEED = 10000\n",
    "  X_train, X_test, y_train, y_test = modelsel.train_test_split(X_rf, y_oversample, test_size = 0.3, random_state = RANSEED)\n",
    "\n",
    "  # Training model\n",
    "  classifier = sklearn.ensemble.RandomForestClassifier(n_estimators = 100, criterion = 'gini', max_depth = 3, min_samples_leaf = 1) # parameters are set as optimized base model\n",
    "  classifier.fit(X_train, y_train)\n",
    "  # Measure model performance\n",
    "  y_model = classifier.predict(X_test)\n",
    "  print(classification_report(y_test, y_model)) # print classification report\n",
    "  print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "  print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "  print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "  print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "  print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "  print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model, normalize = \"true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CXzL9xYPgKL"
   },
   "outputs": [],
   "source": [
    "if model == 13: # This is Random Forest Classifier model with class weights technique\n",
    "  # Feature selection and onehot encoding for Random Forest Classifier\n",
    "  classifier = sklearn.ensemble.RandomForestClassifier(n_estimators = 2, criterion = 'gini', max_depth = 3, min_samples_leaf = 1)\n",
    "  selected_features=feature_sel(classifier,3,X,y)\n",
    "  X_selected=X[selected_features]\n",
    "  one_hot_X=pd.get_dummies(X_selected,columns=selected_features)\n",
    "  X_rf=one_hot_X.to_numpy()\n",
    "  # Partition into training/ test/ validation\n",
    "  RANSEED = 10000\n",
    "  X_train, X_test, y_train, y_test = modelsel.train_test_split(X_rf, y, test_size = 0.3, random_state = RANSEED)\n",
    "\n",
    "  # Training model\n",
    "  classifier = sklearn.ensemble.RandomForestClassifier(n_estimators = 100, criterion = 'gini', max_depth = 3, min_samples_leaf = 1,class_weight=('balanced'))\n",
    "  start1 = time.time()\n",
    "  classifier.fit(X_train, y_train)\n",
    "  stop1 = time.time()\n",
    "\n",
    "# Measure model performance\n",
    "\n",
    "  start2 = time.time()\n",
    "  y_model = classifier.predict(X_test)\n",
    "  stop2 = time.time()\n",
    "  training_time = stop1 - start1\n",
    "  testing_time = stop2 - start2\n",
    "  print(classification_report(y_test, y_model)) # print classification report\n",
    "  print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "  print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "  print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "  print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "  print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "  print(training_time)\n",
    "  print(testing_time)\n",
    "  cm=sklearn.metrics.confusion_matrix(y_test, y_model,normalize='true')\n",
    "  print(cm)\n",
    "  ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xliHES8Wp2ST"
   },
   "outputs": [],
   "source": [
    "#Author: Radhey Krishna Adhikari (Decision Tree Classifier all methods)\n",
    "if model == 21: # This is Decision Tree Classifier model without balancing techniques\n",
    "  # Partition into training/ test/ validation\n",
    "  RANSEED = 10000\n",
    "  X_train, X_test, y_train, y_test = modelsel.train_test_split(X, y, test_size = 0.3, random_state = RANSEED)\n",
    "\n",
    "  # Defining possible values of parameters \n",
    "  parameters = {'min_samples_leaf': [1, 2], \n",
    "                'max_depth': [3, 4, 5],\n",
    "                'criterion': ['gini', 'entropy']} \n",
    "  DTC = tree.DecisionTreeClassifier()\n",
    "  classifier = GridSearchCV(DTC, parameters)\n",
    "  # Fitting the model \n",
    "  classifier.fit(X_train, y_train)\n",
    "\n",
    "  # print best parameter \n",
    "  print(classifier.best_params_)\n",
    "\n",
    "  # print model\n",
    "  print(classifier.best_estimator_)\n",
    "    \n",
    "  # Measure model performance\n",
    "  y_model = classifier.predict(X_test)\n",
    "  print(classification_report(y_test, y_model)) # print classification report\n",
    "  print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "  print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "  print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "  print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "  print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "  print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model, normalize = \"true\"))\n",
    "  # Recall the chosen classifier to print decision tree\n",
    "  classifier = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, min_samples_leaf = 1)\n",
    "  classifier.fit(X_train, y_train)\n",
    "  classifier_predictions = classifier.predict(X_test)\n",
    "\n",
    "  # Plot decision tree\n",
    "  featurenames = df.drop(['is_goal'], axis=1).columns.values.tolist()\n",
    "  classnames = [\"0\", \"1\"]\n",
    "\n",
    "  dot_data = tree.export_graphviz(classifier, out_file=None,feature_names=featurenames,\n",
    "                              impurity=True, class_names=classnames, filled=True, rounded=True, special_characters=True)\n",
    "  graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "  colors = ('lightblue', 'green')\n",
    "  edges = collections.defaultdict(list)\n",
    "  for edge in graph.get_edge_list():\n",
    "    edges[edge.get_source()].append(int(edge.get_destination()))\n",
    "  for edge in edges:\n",
    "    edges[edge].sort()\n",
    "  for i in range(2):\n",
    "      dest = graph.get_node(str(edges[edge][i]))[0]\n",
    "      dest.set_fillcolor(colors[i])\n",
    "  filename = \"conttree_project.png\"\n",
    "  graph.write_png(filename)\n",
    "  from IPython.display import Image\n",
    "  from IPython.core.display import HTML \n",
    "  PATH = \"conttree_project.png\"\n",
    "  Image(filename = PATH , width=500, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5bXTdW3nGLy"
   },
   "outputs": [],
   "source": [
    "if model == 22: # This is Decision Tree Classifier model with oversampling technique\n",
    "  # Partition into training/ test/ validation\n",
    "  RANSEED = 10000\n",
    "  X_train, X_test, y_train, y_test = modelsel.train_test_split(X_oversample, y_oversample, test_size = 0.3, random_state = RANSEED)\n",
    "  classifier = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, min_samples_leaf = 1) # parameters are set as optimized base model\n",
    "\n",
    "  #Feature Selection: For decision tree classifier, the algorithm naturally choose variables\n",
    "  # Training model\n",
    "  classifier.fit(X_train, y_train)\n",
    "\n",
    "  \n",
    "  # Measure model performance\n",
    "  y_model = classifier.predict(X_test)\n",
    "  print(classification_report(y_test, y_model)) # print classification report\n",
    "  print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "  print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "  print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "  print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "  print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "  print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model, normalize = \"true\"))\n",
    "  # Plot decision tree\n",
    "  featurenames = df.drop(['is_goal'], axis=1).columns.values.tolist()\n",
    "  classnames = [\"0\", \"1\"]\n",
    "\n",
    "  dot_data = tree.export_graphviz(classifier, out_file=None,feature_names=featurenames,\n",
    "                              impurity=True, class_names=classnames, filled=True, rounded=True, special_characters=True)\n",
    "  graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "  colors = ('lightblue', 'green')\n",
    "  edges = collections.defaultdict(list)\n",
    "  for edge in graph.get_edge_list():\n",
    "    edges[edge.get_source()].append(int(edge.get_destination()))\n",
    "  for edge in edges:\n",
    "    edges[edge].sort()\n",
    "  for i in range(2):\n",
    "      dest = graph.get_node(str(edges[edge][i]))[0]\n",
    "      dest.set_fillcolor(colors[i])\n",
    "  filename = \"conttree_project.png\"\n",
    "  graph.write_png(filename)\n",
    "  from IPython.display import Image\n",
    "  from IPython.core.display import HTML \n",
    "  PATH = \"conttree_project.png\"\n",
    "  Image(filename = PATH , width=500, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 791
    },
    "executionInfo": {
     "elapsed": 1489,
     "status": "ok",
     "timestamp": 1670374148463,
     "user": {
      "displayName": "Sanchit Sethi",
      "userId": "07740885565284815999"
     },
     "user_tz": 300
    },
    "id": "aK991n_0XmNB",
    "outputId": "c60de828-0e27-44d0-9553-6f618c5d6088"
   },
   "outputs": [],
   "source": [
    "if model == 23: # This is Decision Tree Classifier model with class weights technique\n",
    "  # Partition into training/ test/ validation\n",
    "  RANSEED = 10000\n",
    "  X_train, X_test, y_train, y_test = modelsel.train_test_split(X, y, test_size = 0.3, random_state = RANSEED)\n",
    "  classifier = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, min_samples_leaf = 1, class_weight=('balanced')) # parameters are set as optimized base model\n",
    "  #Feature Selection: For decision tree classifier, the algorithm naturally choose variables\n",
    "\n",
    "  start1 = time.time()\n",
    "  classifier.fit(X_train, y_train)\n",
    "  stop1 = time.time()\n",
    "# Measure model performance\n",
    "\n",
    "  start2 = time.time()\n",
    "  y_model = classifier.predict(X_test)\n",
    "  stop2 = time.time()\n",
    "  training_time = stop1 - start1\n",
    "  testing_time = stop2 - start2\n",
    "\n",
    "  print(classification_report(y_test, y_model)) # print classification report\n",
    "  print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "  print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "  print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "  print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "  print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "  print(training_time)\n",
    "  print(testing_time)\n",
    "  cm=sklearn.metrics.confusion_matrix(y_test, y_model,normalize='true')\n",
    "  print(cm)\n",
    "  ConfusionMatrixDisplay(cm).plot()\n",
    "  # Plot decision tree\n",
    "featurenames = df.drop(['is_goal'], axis=1).columns.values.tolist()\n",
    "classnames = [\"0\", \"1\"]\n",
    "\n",
    "dot_data = tree.export_graphviz(classifier, out_file=None,feature_names=featurenames,\n",
    "                            impurity=True, class_names=classnames, filled=True, rounded=True, special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "colors = ('lightblue', 'green')\n",
    "edges = collections.defaultdict(list)\n",
    "for edge in graph.get_edge_list():\n",
    "  edges[edge.get_source()].append(int(edge.get_destination()))\n",
    "for edge in edges:\n",
    "  edges[edge].sort()\n",
    "for i in range(2):\n",
    "    dest = graph.get_node(str(edges[edge][i]))[0]\n",
    "    dest.set_fillcolor(colors[i])\n",
    "filename = \"conttree_project.png\"\n",
    "graph.write_png(filename)\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "PATH = \"conttree_project.png\"\n",
    "Image(filename = PATH , width=500, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skE9qxUdvygY"
   },
   "outputs": [],
   "source": [
    "#Author: Sanchit Sethi (MLP Classifier all methods)\n",
    "if model == 31: # This is the MLP without balacing techniques\n",
    "  epochs = 20\n",
    "# Feature selection and onehot encoding for MLP\n",
    "  classifier = MLPClassifier(hidden_layer_sizes = (2,2), solver = 'adam', random_state = 24060, activation = 'tanh', \\\n",
    "                    alpha = 0.001, max_iter= 1000)\n",
    "  selected_features=feature_sel(classifier,3,X,y)\n",
    "  X_selected=X[selected_features]\n",
    "  one_hot_X=pd.get_dummies(X_selected,columns=selected_features)\n",
    "  X_mlp=one_hot_X.to_numpy()\n",
    "  # Partition into training/ test/ validation\n",
    "  RANSEED = 10000\n",
    "  X_train, X_test, y_train, y_test = modelsel.train_test_split(X_mlp, y, test_size = 0.3, random_state = RANSEED)\n",
    "  # Partition the dataset\n",
    "  def tfANNModel(): # using the keras API of TensorFlow\n",
    "    inputwidth = X_train.shape[1]\n",
    "    tf.random.set_seed(24060)\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(inputwidth,)),\n",
    "    tf.keras.layers.Dense(6, activation= 'tanh'),\n",
    "    tf.keras.layers.Dense(6, activation= 'tanh'),\n",
    "    tf.keras.layers.Dense(1, activation='relu'),\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "  model = tfANNModel()\n",
    "  history = model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0) #This should be fit on the whole X and Y I think\n",
    "  y_model = model.predict(X_test)\n",
    "  y_model_bin = 1*(y_model > 0.5)\n",
    "\n",
    "  print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model_bin)))\n",
    "  print('Precision is {}'.format(metrics.precision_score(y_test, y_model_bin))) \n",
    "  print('Recall is {}'.format(metrics.recall_score(y_test, y_model_bin)))\n",
    "  print('F1 score is {}'.format(metrics.f1_score(y_test, y_model_bin))) \n",
    "  print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model_bin))\n",
    "  print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0lG27-OiHMa"
   },
   "outputs": [],
   "source": [
    "if model == 32: # This is the MLP with oversampling technique\n",
    "  epochs = 20\n",
    "# Feature selection and onehot encoding for MLP\n",
    "  classifier = MLPClassifier(hidden_layer_sizes = (2,2), solver = 'adam', random_state = 24060, activation = 'tanh', \\\n",
    "                    alpha = 0.001, max_iter= 1000)\n",
    "  selected_features=feature_sel(classifier,3,X_oversample,y_oversample)\n",
    "  X_selected=X_oversample[selected_features]\n",
    "  one_hot_X=pd.get_dummies(X_selected,columns=selected_features)\n",
    "  X_mlp=one_hot_X.to_numpy()\n",
    "  # Partition into training/ test/ validation\n",
    "  RANSEED = 10000\n",
    "  X_train, X_test, y_train, y_test = modelsel.train_test_split(X_mlp, y_oversample, test_size = 0.3, random_state = RANSEED)\n",
    "  # Partition the dataset\n",
    "  \n",
    "  def tfANNModel(): # using the keras API of TensorFlow\n",
    "      inputwidth = X_train.shape[1]\n",
    "      tf.random.set_seed(24060)\n",
    "      model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.InputLayer(input_shape=(inputwidth,)),\n",
    "      tf.keras.layers.Dense(6, activation= 'tanh'),\n",
    "      tf.keras.layers.Dense(6, activation= 'tanh'),\n",
    "      tf.keras.layers.Dense(1, activation='relu'),\n",
    "      ])\n",
    "      model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "      return model\n",
    "\n",
    "  model = tfANNModel()\n",
    "  history = model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0) #This should be fit on the whole X and Y I think\n",
    "  y_model = model.predict(X_test)\n",
    "  y_model_bin = 1*(y_model > 0.5)\n",
    "\n",
    "  print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model_bin)))\n",
    "  print('Precision is {}'.format(metrics.precision_score(y_test, y_model_bin))) \n",
    "  print('Recall is {}'.format(metrics.recall_score(y_test, y_model_bin)))\n",
    "  print('F1 score is {}'.format(metrics.f1_score(y_test, y_model_bin))) \n",
    "  print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model_bin))\n",
    "  print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjy1wzj1jUzB"
   },
   "outputs": [],
   "source": [
    "if model == 33: # This is the MLP with class weight technique\n",
    "  epochs = 20\n",
    "# Feature selection and onehot encoding for MLP\n",
    "  classifier = MLPClassifier(hidden_layer_sizes = (2,2), solver = 'adam', random_state = 24060, activation = 'tanh', \\\n",
    "                    alpha = 0.001, max_iter= 1000)\n",
    "  selected_features=feature_sel(classifier,3,X,y)\n",
    "  X_selected=X[selected_features]\n",
    "  one_hot_X=pd.get_dummies(X_selected,columns=selected_features)\n",
    "  X_mlp=one_hot_X.to_numpy()\n",
    "  # Partition into training/ test/ validation\n",
    "  RANSEED = 10000\n",
    "  X_train, X_test, y_train, y_test = modelsel.train_test_split(X_mlp, y, test_size = 0.3, random_state = RANSEED)\n",
    "  # Partition the dataset\n",
    "  \n",
    "  def tfANNModel(): # using the keras API of TensorFlow\n",
    "      inputwidth = X_train.shape[1]\n",
    "      tf.random.set_seed(24060)\n",
    "      model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.InputLayer(input_shape=(inputwidth,)),\n",
    "      tf.keras.layers.Dense(6, activation= 'tanh'),\n",
    "      tf.keras.layers.Dense(6, activation= 'tanh'),\n",
    "      tf.keras.layers.Dense(1, activation='relu'),\n",
    "      ])\n",
    "      model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "      return model\n",
    "\n",
    "  model = tfANNModel()\n",
    "  total = len(df_is_goal_1)+len(df_is_goal_0)\n",
    "  weight_for_0 = (1 / len(df_is_goal_0)) * (total / 2.0)\n",
    "  weight_for_1 = (1 / len(df_is_goal_1)) * (total / 2.0)\n",
    "\n",
    "  class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "  start1 = time.time()\n",
    "  history = model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0, class_weight = class_weights)\n",
    "  stop1 = time.time()\n",
    "\n",
    "  start2 = time.time()\n",
    "  y_model = model.predict(X_test)\n",
    "  stop2 = time.time()\n",
    "  \n",
    "  y_model_bin = 1*(y_model > 0.5)\n",
    "  training_time = stop1 - start1\n",
    "  prediction_time = stop2 - start2\n",
    "\n",
    "  print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model_bin)))\n",
    "\n",
    "  print('Precision is {}'.format(metrics.precision_score(y_test, y_model_bin))) \n",
    "  print('Recall is {}'.format(metrics.recall_score(y_test, y_model_bin)))\n",
    "  print('F1 score is {}'.format(metrics.f1_score(y_test, y_model_bin))) \n",
    "  cm=sklearn.metrics.confusion_matrix(y_test, y_model_bin,normalize='true')\n",
    "  print(cm)\n",
    "  ConfusionMatrixDisplay(cm).plot()\n",
    "  print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "  print(\"Training time is: \", training_time)\n",
    "  print(\"Prediction time is: \", prediction_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLMPq8eM2A46"
   },
   "outputs": [],
   "source": [
    "#Author: Yasir Gamieldien (SVM classifier all methods)\n",
    "if model == 41: # This is SVM model without balancing technique\n",
    "  # Feature selection and onehot encoding for SVM\n",
    "  # Feature selection takes very long but after the features were found, we manually appended the top 3 features\n",
    "  classifier = SVC(kernel = \"rbf\", C=1, gamma = 1)\n",
    "  #selected_features=feature_sel(classifier,3,X,y)\n",
    "  X_selected =X_oversample.drop([\"time\", \"shot_outcome\", \"location\", \"bodypart\",\"assist_method\"],axis=1) #top 3 features selected\n",
    "  one_hot_X=pd.get_dummies(X_selected,columns=selected_features)\n",
    "  X_svm=one_hot_X.to_numpy()\n",
    "  # Partition into training/ test/ validation\n",
    "  RANSEED = 10000\n",
    "  X_train, X_test, y_train, y_test = modelsel.train_test_split(X_svm, y, test_size = 0.3, random_state = RANSEED)\n",
    "\n",
    "  # Training model\n",
    "  classifier.fit(X_train, y_train)\n",
    "  #print(\"SVM Score is %f\" % clf.score(X_train, y_train))\n",
    "  #print(\"W = \", clf.intercept_, clf.coef_)\n",
    "\n",
    "  print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "  print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "  print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "  print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "  print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "  print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model, normalize = \"true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mF0zgkQXpap-"
   },
   "outputs": [],
   "source": [
    "if model == 42: # This is SVM model with oversampling technique\n",
    "  # Feature selection and onehot encoding for SVM\n",
    "  # Feature selection takes very long but after the features were found, we manually appended the top 3 features\n",
    "  classifier = SVC(kernel = \"rbf\", C=1, gamma = 1)\n",
    "  #selected_features=feature_sel(classifier,3,X_oversample,y_oversample)\n",
    "  X_selected =X_oversample.drop([\"time\", \"shot_outcome\", \"location\", \"bodypart\",\"assist_method\"],axis=1) #top 3 features selected\n",
    "  one_hot_X=pd.get_dummies(X_selected)\n",
    "  X_rf=one_hot_X.to_numpy()\n",
    "  # Partition into training/ test/ validation\n",
    "  RANSEED = 10000\n",
    "  X_train, X_test, y_train, y_test = modelsel.train_test_split(X_rf, y_oversample, test_size = 0.3, random_state = RANSEED)\n",
    "\n",
    "  # Training model\n",
    "  classifier = SVC(kernel = \"rbf\")\n",
    "  classifier.fit(X_train, y_train)\n",
    "  # Measure model performance\n",
    "  y_model = classifier.predict(X_test)\n",
    "  print(classification_report(y_test, y_model))\n",
    "\n",
    "  print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "  print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "  print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "  print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "  print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "  print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model, normalize = \"true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFOzoTDdpab8"
   },
   "outputs": [],
   "source": [
    "if model == 43: # This is SVM model with oversampling technique\n",
    "  # Feature selection and onehot encoding for SVM\n",
    "  # Feature selection takes very long but after the features were found, we manually appended the top 3 features\n",
    "  classifier = SVC(kernel = \"rbf\", class_weight='balanced', C=1, gamma = 1)\n",
    "  #selected_features=feature_sel(classifier,3,X_oversample,y_oversample)\n",
    "  X_selected =X.drop([\"time\", \"shot_outcome\", \"location\", \"bodypart\",\"assist_method\"],axis=1) #top 3 features selected\n",
    "  one_hot_X=pd.get_dummies(X_selected)\n",
    "  X_rf=one_hot_X.to_numpy()\n",
    "  # Partition into training/ test/ validation\n",
    "  RANSEED = 10000\n",
    "  X_train, X_test, y_train, y_test = modelsel.train_test_split(X_rf, y, test_size = 0.3, random_state = RANSEED)\n",
    "\n",
    "  # Training model -\n",
    "  classifier = SVC(kernel = \"rbf\")\n",
    "  start1 = time.time()\n",
    "  classifier.fit(X_train, y_train)\n",
    "  stop1 = time.time()\n",
    "  # Measure model performance\n",
    "  start2 = time.time()\n",
    "  y_model = classifier.predict(X_test)\n",
    "  stop2 = time.time()\n",
    "\n",
    "  training_time = stop1 - start1\n",
    "  prediction_time = stop2 - start2\n",
    "\n",
    "  print(classification_report(y_test, y_model))\n",
    "\n",
    "  print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "  print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "  print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "  print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "  print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "  print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model, normalize = \"true\"))\n",
    "\n",
    "  print(\"Training time is: \", training_time)\n",
    "  print(\"Prediction time time is: \", prediction_time)\n",
    "  cm=sklearn.metrics.confusion_matrix(y_test, y_model,normalize='true')\n",
    "  print(cm)\n",
    "\n",
    "  ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "executionInfo": {
     "elapsed": 20237,
     "status": "error",
     "timestamp": 1670373958138,
     "user": {
      "displayName": "Sanchit Sethi",
      "userId": "07740885565284815999"
     },
     "user_tz": 300
    },
    "id": "Y0vh8ldkvyy0",
    "outputId": "c7e75ab3-3bdc-4c49-cce8-645f83295956"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "# defining parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001],\n",
    "              'kernel': ['rbf']} \n",
    "  \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 132,
     "status": "error",
     "timestamp": 1670373961213,
     "user": {
      "displayName": "Sanchit Sethi",
      "userId": "07740885565284815999"
     },
     "user_tz": 300
    },
    "id": "KZCdDgT5vy5W",
    "outputId": "2a58da71-48d6-492e-c783-856ad600cdde"
   },
   "outputs": [],
   "source": [
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "  \n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "error",
     "timestamp": 1670373964038,
     "user": {
      "displayName": "Sanchit Sethi",
      "userId": "07740885565284815999"
     },
     "user_tz": 300
    },
    "id": "TZviCeYtkS4W",
    "outputId": "c2915cbb-573a-489d-8226-4e8e998c1d11"
   },
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5p6D5pJDC6m"
   },
   "outputs": [],
   "source": [
    "#Run this command to ensure matplot lib is updated to the most recent package to avoid getting error from fig.supylabel and fig.supxlabel\n",
    "#pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 886
    },
    "executionInfo": {
     "elapsed": 566,
     "status": "ok",
     "timestamp": 1670374257371,
     "user": {
      "displayName": "Sanchit Sethi",
      "userId": "07740885565284815999"
     },
     "user_tz": 300
    },
    "id": "glYnB1oMAule",
    "outputId": "cbdcef56-e29f-449b-f56e-c3199ab264ce"
   },
   "outputs": [],
   "source": [
    "#Plot for the execution time\n",
    "#make sure matplotlib is upgraded before running this section might throw an error if not upgraded\n",
    "#data manually extracted from time.xlsx file\n",
    "#Author: all team members\n",
    "y_training=[0.03000,0.466667,16.593333,20.243333]\n",
    "y_predi=[0.003333,0.066667,6.970000,0.553333]\n",
    "\n",
    "fig, axs = plt.subplots(2,sharex=True,figsize=(14,12))\n",
    "fig.suptitle('Time Comparisions of different Algorithms')\n",
    "axs[0].bar(x=[\"Decision Tree\",\"Random Forest\",\"SVM\",\"MLP\"],\n",
    "        height=y_training)\n",
    "axs[1].bar(x=[\"Decision Tree\",\"Random Forest\",\"SVM\",\"MLP\"],\n",
    "        height=y_predi)\n",
    "\n",
    "axs[0].set_title(\"Average Training Time\")\n",
    "axs[1].set_title(\"Average Prediction Time\")\n",
    "\n",
    "\n",
    "fig.supylabel(\"Time\")\n",
    "fig.supxlabel(\"Algorithms\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1s3TFFdpcbXMfyswx7kSObhAiNuQ6Em8k",
     "timestamp": 1670285219495
    },
    {
     "file_id": "1Tq0OlXVhsSAxc0u3Kwj0WUtTWEdpSL9F",
     "timestamp": 1669599139104
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
