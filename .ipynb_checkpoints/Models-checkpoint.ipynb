{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BpqG3V19PkS3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 15:37:23.253471: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-25 15:37:23.312428: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-25 15:37:23.315292: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-25 15:37:23.315301: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-25 15:37:23.328720: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-25 15:37:23.594590: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-25 15:37:23.594624: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-25 15:37:23.594635: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Import supporting libraries\n",
    "\n",
    "import pandas \n",
    "import numpy\n",
    "import sklearn.impute as impute\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.model_selection as modelsel\n",
    "import sklearn.tree as tree\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.svm as SVC\n",
    "import pydotplus\n",
    "import collections\n",
    "import sklearn\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "from sklearn.metrics import  ConfusionMatrixDisplay\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"../Goal_prediction_Data/cleaned_dataSPDO.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop([\"is_goal\"],axis=1)\n",
    "y=df[\"is_goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of observations (0s and 1s) in the target varible\n",
    "df_is_goal_1 = df[df[\"is_goal\"]==1]\n",
    "df_is_goal_0 = df[df[\"is_goal\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18720\n",
      "13683\n",
      "32403\n"
     ]
    }
   ],
   "source": [
    "print(len(df_is_goal_0))\n",
    "print(len(df_is_goal_1))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============: IMPLEMENT BALANCING TECHNIQUES TO THE DATASET :================\n",
    "#  Technique 1: Oversampling (technique 2 is combined in the function of training the model)\n",
    "over_sample_1 = resample(df_is_goal_1,n_samples = len(df_is_goal_0)) \n",
    "# The number of observations 0 is bigger than observation 1. \n",
    "# So choose len(df_is_goal_0) is 50% of the number of the total observations in the target variable. \n",
    "df_oversample = pandas.concat([df_is_goal_0, over_sample_1]) # New data set after oversampling\n",
    "\n",
    "df_oversample = df_oversample.sample(frac = 1)                          # Sampling without replacement (basically reshuffling data)\n",
    "\n",
    "# Define predictor features and target features for oversampling dataset\n",
    "X_oversample =df_oversample.drop([\"is_goal\"],axis=1)\n",
    "y_oversample =df_oversample[\"is_goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sequential feature selection method\n",
    "def feature_sel(model_name, n_selected_features, X, y):  \n",
    "    #Feature selection\n",
    "    sfs = SequentialFeatureSelector(model_name, n_features_to_select = n_selected_features)   # Sequential model of sklearn is used\n",
    "    features= X.columns.values\n",
    "    sfs.fit(X,y)\n",
    "    support=sfs.get_support()  \n",
    "    selected_features=features*support\n",
    "    selected_features=list(selected_features)\n",
    "    selected_features=[a for a in selected_features if a!=\"\"]\n",
    "    print(\"Selected Features: \\n\")\n",
    "    print(selected_features)\n",
    "    return(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_and_d(model,X_train,y_train,y_test):\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_model = model.predict(X_test)\n",
    "    print(\"Accuracy score:  {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "    print('Precision:  {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "    print('Recall:  {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "    print('F1 score: {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "    print(\"Confusion matrix is: \\n\", metrics.confusion_matrix(y_test, y_model, normalize = \"true\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (1733736861.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [13]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def fit_model(model,parameters=None, X_train, X_test, y_train, y_test,flag=False):\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def fit_model(model, X_train, X_test, y_train, y_test,parameters=None,flag=False):\n",
    "    \n",
    "    if(flag==1):\n",
    "        print(\"\\n This model is with Oversampling \\n\")\n",
    "        \n",
    "        f_and_d(model,X_train,y_train,y_test)\n",
    "        \n",
    "    elif(flag==2):\n",
    "        print(\"\\n This model is with class weight balancing technique\")\n",
    "        \n",
    "        f_and_d(model,X_train,y_train,y_test)\n",
    "    else:\n",
    "        print(\"\\n This model is without balancing technique \\n\")\n",
    "        classifier = GridSearchCV(model, parameters)\n",
    "        # Fitting the model \n",
    "        classifier.fit(X_train, y_train)\n",
    "        # print best parameter \n",
    "        print(\"Best Parameters from Grid Search: \\n\")\n",
    "        print(classifier.best_params_)\n",
    "\n",
    "        f_and_d(classifier,X_train,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: \n",
      "\n",
      "['shot_outcome', 'situation', 'fast_break']\n",
      "\n",
      " This model is without balancing technique \n",
      "\n",
      "Best Parameters from Grid Search: \n",
      "\n",
      "{'criterion': 'gini', 'max_depth': 3, 'n_estimators': 100}\n",
      "Accuracy score:  0.7488941466927271\n",
      "Precision:  0.9594298245614035\n",
      "Recall:  0.42506679621083315\n",
      "F1 score: 0.5891264096953375\n",
      "Confusion matrix is: \n",
      " [[0.98679515 0.01320485]\n",
      " [0.5749332  0.4250668 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier = sklearn.ensemble.RandomForestClassifier(n_estimators = 2, criterion = 'gini', max_depth = 5, min_samples_leaf = 1)\n",
    "selected_features=feature_sel(classifier,3,X,y)\n",
    "X_selected=X[selected_features]\n",
    "one_hot_X=pd.get_dummies(X_selected,columns=selected_features)\n",
    "X_rf=one_hot_X.to_numpy()\n",
    "\n",
    "# Partition into training/ test/ validation\n",
    "RANSEED = 10000\n",
    "X_train, X_test, y_train, y_test = modelsel.train_test_split(X_rf, y, test_size = 0.3, random_state = RANSEED)\n",
    "\n",
    "# Defining possible values of parameters \n",
    "parameters = { 'n_estimators': [100, 300, 500],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'criterion': ['gini', 'entropy']} \n",
    "RFC = sklearn.ensemble.RandomForestClassifier()\n",
    "\n",
    "\n",
    "fit_model(RFC,parameters, X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MYeCe427PLS3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This model is with Oversampling \n",
      "\n",
      "Accuracy score:  0.7488941466927271\n",
      "Precision:  0.9594298245614035\n",
      "Recall:  0.42506679621083315\n",
      "F1 score: 0.5891264096953375\n",
      "Confusion matrix is: \n",
      " [[0.98679515 0.01320485]\n",
      " [0.5749332  0.4250668 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_selected=X_oversample[selected_features]\n",
    "one_hot_X=pd.get_dummies(X_selected,columns=selected_features)\n",
    "X_rf=one_hot_X.to_numpy()\n",
    "# Partition into training/ test/ validation\n",
    "RANSEED = 10000\n",
    "X_train, X_test, y_train, y_test = modelsel.train_test_split(X_rf, y_oversample, test_size = 0.3, random_state = RANSEED)\n",
    "\n",
    "# Training model\n",
    "classifier = sklearn.ensemble.RandomForestClassifier(n_estimators = 100, criterion = 'gini', max_depth = 3) # parameters are set as optimized base model\n",
    "\n",
    "\n",
    "fit_model(classifier,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CXzL9xYPgKL"
   },
   "outputs": [],
   "source": [
    "# This is Random Forest Classifier model with class weights technique\n",
    "# Feature selection and onehot encoding for Random Forest Classifier\n",
    "classifier = sklearn.ensemble.RandomForestClassifier(n_estimators = 2, criterion = 'gini', max_depth = 3, min_samples_leaf = 1)\n",
    "selected_features=feature_sel(classifier,3,X,y)\n",
    "X_selected=X[selected_features]\n",
    "one_hot_X=pd.get_dummies(X_selected,columns=selected_features)\n",
    "X_rf=one_hot_X.to_numpy()\n",
    "# Partition into training/ test/ validation\n",
    "RANSEED = 10000\n",
    "X_train, X_test, y_train, y_test = modelsel.train_test_split(X_rf, y, test_size = 0.3, random_state = RANSEED)\n",
    "\n",
    "# Training model\n",
    "classifier = sklearn.ensemble.RandomForestClassifier(n_estimators = 100, criterion = 'gini', max_depth = 3, min_samples_leaf = 1,class_weight=('balanced'))\n",
    "start1 = time.time()\n",
    "classifier.fit(X_train, y_train)\n",
    "stop1 = time.time()\n",
    "\n",
    "# Measure model performance\n",
    "\n",
    "start2 = time.time()\n",
    "y_model = classifier.predict(X_test)\n",
    "stop2 = time.time()\n",
    "training_time = stop1 - start1\n",
    "testing_time = stop2 - start2\n",
    "print(classification_report(y_test, y_model)) # print classification report\n",
    "print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "print(training_time)\n",
    "print(testing_time)\n",
    "cm=sklearn.metrics.confusion_matrix(y_test, y_model,normalize='true')\n",
    "print(cm)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xliHES8Wp2ST"
   },
   "outputs": [],
   "source": [
    "# This is Decision Tree Classifier model without balancing techniques\n",
    "# Partition into training/ test/ validation\n",
    "RANSEED = 10000\n",
    "X_train, X_test, y_train, y_test = modelsel.train_test_split(X, y, test_size = 0.3, random_state = RANSEED)\n",
    "\n",
    "# Defining possible values of parameters \n",
    "parameters = {'min_samples_leaf': [1, 2], \n",
    "            'max_depth': [3, 4, 5],\n",
    "            'criterion': ['gini', 'entropy']} \n",
    "DTC = tree.DecisionTreeClassifier()\n",
    "classifier = GridSearchCV(DTC, parameters)\n",
    "# Fitting the model \n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# print best parameter \n",
    "print(classifier.best_params_)\n",
    "\n",
    "# print model\n",
    "print(classifier.best_estimator_)\n",
    "\n",
    "# Measure model performance\n",
    "y_model = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_model)) # print classification report\n",
    "print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model, normalize = \"true\"))\n",
    "# Recall the chosen classifier to print decision tree\n",
    "classifier = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, min_samples_leaf = 1)\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier_predictions = classifier.predict(X_test)\n",
    "\n",
    "# Plot decision tree\n",
    "featurenames = df.drop(['is_goal'], axis=1).columns.values.tolist()\n",
    "classnames = [\"0\", \"1\"]\n",
    "\n",
    "dot_data = tree.export_graphviz(classifier, out_file=None,feature_names=featurenames,\n",
    "                          impurity=True, class_names=classnames, filled=True, rounded=True, special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "colors = ('lightblue', 'green')\n",
    "edges = collections.defaultdict(list)\n",
    "for edge in graph.get_edge_list():\n",
    "edges[edge.get_source()].append(int(edge.get_destination()))\n",
    "for edge in edges:\n",
    "edges[edge].sort()\n",
    "for i in range(2):\n",
    "  dest = graph.get_node(str(edges[edge][i]))[0]\n",
    "  dest.set_fillcolor(colors[i])\n",
    "filename = \"conttree_project.png\"\n",
    "graph.write_png(filename)\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "PATH = \"conttree_project.png\"\n",
    "Image(filename = PATH , width=500, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5bXTdW3nGLy"
   },
   "outputs": [],
   "source": [
    "# This is Decision Tree Classifier model with oversampling technique\n",
    "# Partition into training/ test/ validation\n",
    "RANSEED = 10000\n",
    "X_train, X_test, y_train, y_test = modelsel.train_test_split(X_oversample, y_oversample, test_size = 0.3, random_state = RANSEED)\n",
    "classifier = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, min_samples_leaf = 1) # parameters are set as optimized base model\n",
    "\n",
    "#Feature Selection: For decision tree classifier, the algorithm naturally choose variables\n",
    "# Training model\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Measure model performance\n",
    "y_model = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_model)) # print classification report\n",
    "print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model, normalize = \"true\"))\n",
    "# Plot decision tree\n",
    "featurenames = df.drop(['is_goal'], axis=1).columns.values.tolist()\n",
    "classnames = [\"0\", \"1\"]\n",
    "\n",
    "dot_data = tree.export_graphviz(classifier, out_file=None,feature_names=featurenames,\n",
    "                          impurity=True, class_names=classnames, filled=True, rounded=True, special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "colors = ('lightblue', 'green')\n",
    "edges = collections.defaultdict(list)\n",
    "for edge in graph.get_edge_list():\n",
    "edges[edge.get_source()].append(int(edge.get_destination()))\n",
    "for edge in edges:\n",
    "edges[edge].sort()\n",
    "for i in range(2):\n",
    "  dest = graph.get_node(str(edges[edge][i]))[0]\n",
    "  dest.set_fillcolor(colors[i])\n",
    "filename = \"conttree_project.png\"\n",
    "graph.write_png(filename)\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "PATH = \"conttree_project.png\"\n",
    "Image(filename = PATH , width=500, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 791
    },
    "executionInfo": {
     "elapsed": 1489,
     "status": "ok",
     "timestamp": 1670374148463,
     "user": {
      "displayName": "Sanchit Sethi",
      "userId": "07740885565284815999"
     },
     "user_tz": 300
    },
    "id": "aK991n_0XmNB",
    "outputId": "c60de828-0e27-44d0-9553-6f618c5d6088"
   },
   "outputs": [],
   "source": [
    "# This is Decision Tree Classifier model with class weights technique\n",
    "# Partition into training/ test/ validation\n",
    "RANSEED = 10000\n",
    "X_train, X_test, y_train, y_test = modelsel.train_test_split(X, y, test_size = 0.3, random_state = RANSEED)\n",
    "classifier = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, min_samples_leaf = 1, class_weight=('balanced')) # parameters are set as optimized base model\n",
    "#Feature Selection: For decision tree classifier, the algorithm naturally choose variables\n",
    "\n",
    "start1 = time.time()\n",
    "classifier.fit(X_train, y_train)\n",
    "stop1 = time.time()\n",
    "# Measure model performance\n",
    "\n",
    "start2 = time.time()\n",
    "y_model = classifier.predict(X_test)\n",
    "stop2 = time.time()\n",
    "training_time = stop1 - start1\n",
    "testing_time = stop2 - start2\n",
    "\n",
    "print(classification_report(y_test, y_model)) # print classification report\n",
    "print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "print(training_time)\n",
    "print(testing_time)\n",
    "cm=sklearn.metrics.confusion_matrix(y_test, y_model,normalize='true')\n",
    "print(cm)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "# Plot decision tree\n",
    "featurenames = df.drop(['is_goal'], axis=1).columns.values.tolist()\n",
    "classnames = [\"0\", \"1\"]\n",
    "\n",
    "dot_data = tree.export_graphviz(classifier, out_file=None,feature_names=featurenames,\n",
    "                        impurity=True, class_names=classnames, filled=True, rounded=True, special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "colors = ('lightblue', 'green')\n",
    "edges = collections.defaultdict(list)\n",
    "for edge in graph.get_edge_list():\n",
    "edges[edge.get_source()].append(int(edge.get_destination()))\n",
    "for edge in edges:\n",
    "edges[edge].sort()\n",
    "for i in range(2):\n",
    "dest = graph.get_node(str(edges[edge][i]))[0]\n",
    "dest.set_fillcolor(colors[i])\n",
    "filename = \"conttree_project.png\"\n",
    "graph.write_png(filename)\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "PATH = \"conttree_project.png\"\n",
    "Image(filename = PATH , width=500, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skE9qxUdvygY"
   },
   "outputs": [],
   "source": [
    "# This is the MLP without balacing techniques\n",
    "epochs = 20\n",
    "# Feature selection and onehot encoding for MLP\n",
    "classifier = MLPClassifier(hidden_layer_sizes = (2,2), solver = 'adam', random_state = 24060, activation = 'tanh', \\\n",
    "                alpha = 0.001, max_iter= 1000)\n",
    "selected_features=feature_sel(classifier,3,X,y)\n",
    "X_selected=X[selected_features]\n",
    "one_hot_X=pd.get_dummies(X_selected,columns=selected_features)\n",
    "X_mlp=one_hot_X.to_numpy()\n",
    "# Partition into training/ test/ validation\n",
    "RANSEED = 10000\n",
    "X_train, X_test, y_train, y_test = modelsel.train_test_split(X_mlp, y, test_size = 0.3, random_state = RANSEED)\n",
    "# Partition the dataset\n",
    "def tfANNModel(): # using the keras API of TensorFlow\n",
    "inputwidth = X_train.shape[1]\n",
    "tf.random.set_seed(24060)\n",
    "model = tf.keras.models.Sequential([\n",
    "tf.keras.layers.InputLayer(input_shape=(inputwidth,)),\n",
    "tf.keras.layers.Dense(6, activation= 'tanh'),\n",
    "tf.keras.layers.Dense(6, activation= 'tanh'),\n",
    "tf.keras.layers.Dense(1, activation='relu'),\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "return model\n",
    "\n",
    "model = tfANNModel()\n",
    "history = model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0) #This should be fit on the whole X and Y I think\n",
    "y_model = model.predict(X_test)\n",
    "y_model_bin = 1*(y_model > 0.5)\n",
    "\n",
    "print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model_bin)))\n",
    "print('Precision is {}'.format(metrics.precision_score(y_test, y_model_bin))) \n",
    "print('Recall is {}'.format(metrics.recall_score(y_test, y_model_bin)))\n",
    "print('F1 score is {}'.format(metrics.f1_score(y_test, y_model_bin))) \n",
    "print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model_bin))\n",
    "print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0lG27-OiHMa"
   },
   "outputs": [],
   "source": [
    "# This is the MLP with oversampling technique\n",
    "epochs = 20\n",
    "# Feature selection and onehot encoding for MLP\n",
    "classifier = MLPClassifier(hidden_layer_sizes = (2,2), solver = 'adam', random_state = 24060, activation = 'tanh', \\\n",
    "                alpha = 0.001, max_iter= 1000)\n",
    "selected_features=feature_sel(classifier,3,X_oversample,y_oversample)\n",
    "X_selected=X_oversample[selected_features]\n",
    "one_hot_X=pd.get_dummies(X_selected,columns=selected_features)\n",
    "X_mlp=one_hot_X.to_numpy()\n",
    "# Partition into training/ test/ validation\n",
    "RANSEED = 10000\n",
    "X_train, X_test, y_train, y_test = modelsel.train_test_split(X_mlp, y_oversample, test_size = 0.3, random_state = RANSEED)\n",
    "# Partition the dataset\n",
    "\n",
    "def tfANNModel(): # using the keras API of TensorFlow\n",
    "  inputwidth = X_train.shape[1]\n",
    "  tf.random.set_seed(24060)\n",
    "  model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(inputwidth,)),\n",
    "  tf.keras.layers.Dense(6, activation= 'tanh'),\n",
    "  tf.keras.layers.Dense(6, activation= 'tanh'),\n",
    "  tf.keras.layers.Dense(1, activation='relu'),\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "  return model\n",
    "\n",
    "model = tfANNModel()\n",
    "history = model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0) #This should be fit on the whole X and Y I think\n",
    "y_model = model.predict(X_test)\n",
    "y_model_bin = 1*(y_model > 0.5)\n",
    "\n",
    "print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model_bin)))\n",
    "print('Precision is {}'.format(metrics.precision_score(y_test, y_model_bin))) \n",
    "print('Recall is {}'.format(metrics.recall_score(y_test, y_model_bin)))\n",
    "print('F1 score is {}'.format(metrics.f1_score(y_test, y_model_bin))) \n",
    "print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model_bin))\n",
    "print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjy1wzj1jUzB"
   },
   "outputs": [],
   "source": [
    "# This is the MLP with class weight technique\n",
    "epochs = 20\n",
    "# Feature selection and onehot encoding for MLP\n",
    "classifier = MLPClassifier(hidden_layer_sizes = (2,2), solver = 'adam', random_state = 24060, activation = 'tanh', \\\n",
    "                alpha = 0.001, max_iter= 1000)\n",
    "selected_features=feature_sel(classifier,3,X,y)\n",
    "X_selected=X[selected_features]\n",
    "one_hot_X=pd.get_dummies(X_selected,columns=selected_features)\n",
    "X_mlp=one_hot_X.to_numpy()\n",
    "# Partition into training/ test/ validation\n",
    "RANSEED = 10000\n",
    "X_train, X_test, y_train, y_test = modelsel.train_test_split(X_mlp, y, test_size = 0.3, random_state = RANSEED)\n",
    "# Partition the dataset\n",
    "\n",
    "def tfANNModel():\n",
    "    inputwidth = X_train.shape[1]\n",
    "    tf.random.set_seed(24060)\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(inputwidth,)),\n",
    "    tf.keras.layers.Dense(6, activation= 'tanh'),\n",
    "    tf.keras.layers.Dense(6, activation= 'tanh'),\n",
    "    tf.keras.layers.Dense(1, activation='relu'),\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = tfANNModel()\n",
    "total = len(df_is_goal_1)+len(df_is_goal_0)\n",
    "weight_for_0 = (1 / len(df_is_goal_0)) * (total / 2.0)\n",
    "weight_for_1 = (1 / len(df_is_goal_1)) * (total / 2.0)\n",
    "\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "start1 = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0, class_weight = class_weights)\n",
    "stop1 = time.time()\n",
    "\n",
    "start2 = time.time()\n",
    "y_model = model.predict(X_test)\n",
    "stop2 = time.time()\n",
    "\n",
    "y_model_bin = 1*(y_model > 0.5)\n",
    "training_time = stop1 - start1\n",
    "prediction_time = stop2 - start2\n",
    "\n",
    "print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model_bin)))\n",
    "\n",
    "print('Precision is {}'.format(metrics.precision_score(y_test, y_model_bin))) \n",
    "print('Recall is {}'.format(metrics.recall_score(y_test, y_model_bin)))\n",
    "print('F1 score is {}'.format(metrics.f1_score(y_test, y_model_bin))) \n",
    "cm=sklearn.metrics.confusion_matrix(y_test, y_model_bin,normalize='true')\n",
    "print(cm)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "print(\"Training time is: \", training_time)\n",
    "print(\"Prediction time is: \", prediction_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLMPq8eM2A46"
   },
   "outputs": [],
   "source": [
    "# This is SVM model without balancing technique\n",
    "# Feature selection and onehot encoding for SVM\n",
    "# Feature selection takes very long but after the features were found, we manually appended the top 3 features\n",
    "classifier = SVC(kernel = \"rbf\", C=1, gamma = 1)\n",
    "#selected_features=feature_sel(classifier,3,X,y)\n",
    "X_selected =X_oversample.drop([\"time\", \"shot_outcome\", \"location\", \"bodypart\",\"assist_method\"],axis=1) #top 3 features selected\n",
    "one_hot_X=pd.get_dummies(X_selected,columns=selected_features)\n",
    "X_svm=one_hot_X.to_numpy()\n",
    "# Partition into training/ test/ validation\n",
    "RANSEED = 10000\n",
    "X_train, X_test, y_train, y_test = modelsel.train_test_split(X_svm, y, test_size = 0.3, random_state = RANSEED)\n",
    "\n",
    "# Training model\n",
    "classifier.fit(X_train, y_train)\n",
    "#print(\"SVM Score is %f\" % clf.score(X_train, y_train))\n",
    "#print(\"W = \", clf.intercept_, clf.coef_)\n",
    "\n",
    "print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model, normalize = \"true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mF0zgkQXpap-"
   },
   "outputs": [],
   "source": [
    "# This is SVM model with oversampling technique\n",
    "# Feature selection and onehot encoding for SVM\n",
    "# Feature selection takes very long but after the features were found, we manually appended the top 3 features\n",
    "classifier = SVC(kernel = \"rbf\", C=1, gamma = 1)\n",
    "#selected_features=feature_sel(classifier,3,X_oversample,y_oversample)\n",
    "X_selected =X_oversample.drop([\"time\", \"shot_outcome\", \"location\", \"bodypart\",\"assist_method\"],axis=1) #top 3 features selected\n",
    "one_hot_X=pd.get_dummies(X_selected)\n",
    "X_rf=one_hot_X.to_numpy()\n",
    "# Partition into training/ test/ validation\n",
    "RANSEED = 10000\n",
    "X_train, X_test, y_train, y_test = modelsel.train_test_split(X_rf, y_oversample, test_size = 0.3, random_state = RANSEED)\n",
    "\n",
    "# Training model\n",
    "classifier = SVC(kernel = \"rbf\")\n",
    "classifier.fit(X_train, y_train)\n",
    "# Measure model performance\n",
    "y_model = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_model))\n",
    "\n",
    "print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model, normalize = \"true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFOzoTDdpab8"
   },
   "outputs": [],
   "source": [
    "# This is SVM model with oversampling technique\n",
    "# Feature selection and onehot encoding for SVM\n",
    "# Feature selection takes very long but after the features were found, we manually appended the top 3 features\n",
    "classifier = SVC(kernel = \"rbf\", class_weight='balanced', C=1, gamma = 1)\n",
    "#selected_features=feature_sel(classifier,3,X_oversample,y_oversample)\n",
    "X_selected =X.drop([\"time\", \"shot_outcome\", \"location\", \"bodypart\",\"assist_method\"],axis=1) #top 3 features selected\n",
    "one_hot_X=pd.get_dummies(X_selected)\n",
    "X_rf=one_hot_X.to_numpy()\n",
    "# Partition into training/ test/ validation\n",
    "RANSEED = 10000\n",
    "X_train, X_test, y_train, y_test = modelsel.train_test_split(X_rf, y, test_size = 0.3, random_state = RANSEED)\n",
    "\n",
    "# Training model -\n",
    "classifier = SVC(kernel = \"rbf\")\n",
    "start1 = time.time()\n",
    "classifier.fit(X_train, y_train)\n",
    "stop1 = time.time()\n",
    "# Measure model performance\n",
    "start2 = time.time()\n",
    "y_model = classifier.predict(X_test)\n",
    "stop2 = time.time()\n",
    "\n",
    "training_time = stop1 - start1\n",
    "prediction_time = stop2 - start2\n",
    "\n",
    "print(classification_report(y_test, y_model))\n",
    "\n",
    "print(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, y_model)))\n",
    "print('Precision is {}'.format(metrics.precision_score(y_test, y_model))) \n",
    "print('Recall is {}'.format(metrics.recall_score(y_test, y_model)))\n",
    "print('F1 score is {}'.format(metrics.f1_score(y_test, y_model))) \n",
    "print(\"Cross-validation scores is:\", sklearn.model_selection.cross_val_score(classifier, X, y=y, cv=5))\n",
    "print(\"Confusion matrix is:\\n\", metrics.confusion_matrix(y_test, y_model, normalize = \"true\"))\n",
    "\n",
    "print(\"Training time is: \", training_time)\n",
    "print(\"Prediction time time is: \", prediction_time)\n",
    "cm=sklearn.metrics.confusion_matrix(y_test, y_model,normalize='true')\n",
    "print(cm)\n",
    "\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "executionInfo": {
     "elapsed": 20237,
     "status": "error",
     "timestamp": 1670373958138,
     "user": {
      "displayName": "Sanchit Sethi",
      "userId": "07740885565284815999"
     },
     "user_tz": 300
    },
    "id": "Y0vh8ldkvyy0",
    "outputId": "c7e75ab3-3bdc-4c49-cce8-645f83295956"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "# defining parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001],\n",
    "              'kernel': ['rbf']} \n",
    "  \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 132,
     "status": "error",
     "timestamp": 1670373961213,
     "user": {
      "displayName": "Sanchit Sethi",
      "userId": "07740885565284815999"
     },
     "user_tz": 300
    },
    "id": "KZCdDgT5vy5W",
    "outputId": "2a58da71-48d6-492e-c783-856ad600cdde"
   },
   "outputs": [],
   "source": [
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "  \n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "error",
     "timestamp": 1670373964038,
     "user": {
      "displayName": "Sanchit Sethi",
      "userId": "07740885565284815999"
     },
     "user_tz": 300
    },
    "id": "TZviCeYtkS4W",
    "outputId": "c2915cbb-573a-489d-8226-4e8e998c1d11"
   },
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5p6D5pJDC6m"
   },
   "outputs": [],
   "source": [
    "#Run this command to ensure matplot lib is updated to the most recent package to avoid getting error from fig.supylabel and fig.supxlabel\n",
    "#pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 886
    },
    "executionInfo": {
     "elapsed": 566,
     "status": "ok",
     "timestamp": 1670374257371,
     "user": {
      "displayName": "Sanchit Sethi",
      "userId": "07740885565284815999"
     },
     "user_tz": 300
    },
    "id": "glYnB1oMAule",
    "outputId": "cbdcef56-e29f-449b-f56e-c3199ab264ce"
   },
   "outputs": [],
   "source": [
    "#Plot for the execution time\n",
    "#make sure matplotlib is upgraded before running this section might throw an error if not upgraded\n",
    "#data manually extracted from time.xlsx file\n",
    "#Author: all team members\n",
    "y_training=[0.03000,0.466667,16.593333,20.243333]\n",
    "y_predi=[0.003333,0.066667,6.970000,0.553333]\n",
    "\n",
    "fig, axs = plt.subplots(2,sharex=True,figsize=(14,12))\n",
    "fig.suptitle('Time Comparisions of different Algorithms')\n",
    "axs[0].bar(x=[\"Decision Tree\",\"Random Forest\",\"SVM\",\"MLP\"],\n",
    "        height=y_training)\n",
    "axs[1].bar(x=[\"Decision Tree\",\"Random Forest\",\"SVM\",\"MLP\"],\n",
    "        height=y_predi)\n",
    "\n",
    "axs[0].set_title(\"Average Training Time\")\n",
    "axs[1].set_title(\"Average Prediction Time\")\n",
    "\n",
    "\n",
    "fig.supylabel(\"Time\")\n",
    "fig.supxlabel(\"Algorithms\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1s3TFFdpcbXMfyswx7kSObhAiNuQ6Em8k",
     "timestamp": 1670285219495
    },
    {
     "file_id": "1Tq0OlXVhsSAxc0u3Kwj0WUtTWEdpSL9F",
     "timestamp": 1669599139104
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
